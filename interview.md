1. Transformer 模型的基本结构是什么，它如何改变深度学习领域的？
a. en+de, en_emb -> enblock -> attn addnormffn addnorm 
          de_emb -> deblock -> attn addnorm attn addnorm ffn addnorm 
b. 更长的序列提取办法，更长期记忆，更大的参数容量，只要有足够多的数据，都能学

2. Transformer 为何能够有效地处理长距离依赖问题？ 与传统RNN和LSTM相比有哪些优势？
a. 因为它每次每句话里面的所有词都会做其他词的注意力评分 ？？
b. 传统RNN和LSTM需要顺序处理序列，但是Transformer可以并行处理序列，所以可以处理更长的序列

3. 多头注意力的作用是什么？
a. 多头注意力机制可以提升模型的参数量，而且不同注意力头可能可以关注到不同的特征，模型会学到更多

4. 能不能手写attention？
a. attention = softmax(Q*K^T/sqrt(d_k))*V

5. Transformer 模型如何平衡模型性能和计算资源的消耗？
a. ？？ *根据任务需要，如果需要更好的模型性能则需要更多的head和更多的堆叠，以及更大的参数量，如果需要更好的计算效率，则反之*

6. Transformer 模型的自注意力机制如何实现并行处理？
a. 每个词的预测是会通过一次decoder块，而且每个词对应的下一个词是知道的，因此可以并行处理

7. 在Transformer模型中，位置编码(positional encoding)的作用是什么, 它是如何工作的？
a. 位置编码的作用是给每个词一个位置信息，因为Transformer模型没有RNN，所以需要位置编码来告诉模型每个词的位置
b. 把位置编码和embedding相加

8. Transformer 模型如何处理变长的输入序列？
a. 通过pad，把所有输入的句子变成相同长度，然后通过mask，告诉模型哪些是pad，哪些是真实值

9. Transformer 模型的缩放点积注意力是什么，它的重要性在哪里？
a. 缩放点积注意力指的是QK^T相乘之后要除的根号d_k，它的重要性在于把点积的值限制在-1，1的分布之间，这样进入softmax之后，梯度才会存在，如果不这么做，数据分布很散，很离散，softmax之后，只会有一个接近1，其它都接近0，梯度就会很小，模型就很难训练（梯度消失）

10. Transformer 模型在实践中如何优化以处理长序列？
？？*模型虽然在序列建模效果很好，但是自注意力的计算复杂度是On2，所以对应有一些优化措施，kvcache是优化推理时反复计算前面词的注意力分数，稀疏注意力机制，只计算部分词与词之间的注意力分数，常见的有固定窗口、随机注意力，还有就是将长序列分块成短序列，模型量化和蒸馏*

11. Transformer 模型在自注意力层中如何解决多尺度表示问题？
？？*多尺度表示指的是变长序列吗？如果是就是使用pad进行补全，然后mask掉补全的pad让模型看不到，如果不是，那是指的多特征，那就是多头注意力，学习不同的特征*

12. Transformer 模型中的自注意力机制在计算效率和表示能力之间是如何权衡的？
？？-- 5

13. Transformer 模型的参数共享策略对模型性能有何影响？
a. 参数共享可以减少参数量，但是模型性能会下降，因为参数共享会限制模型的表达能力

14. Transformer encoder和decoder的结构有什么不同？
a. encoder只有一个自注意力层，decoder有两个，而且decoder的第二个自注意力层会用到encoder的输出，然后encoder没有后续的linear层和softmax层，decoder有

15. Transformer 模型中的前馈网络(feed-forward network)是什么，它的作用是什么？
a. 前馈网络是linear+relu+linear, 作用是提升模型的参数量，可以学习更多信息，并进行非线性激活，**负责对每个位置的特征进行独立的非线性变换和提取，增强模型表达能力（自注意力机制是为了捕捉序列中不同的位置之间关系）**

16. Pre-norm 和 post-norm的区别是什么？
？？

17. Transformer 网络很深，是怎么避免过拟合问题的？
a. 每个自注意力层都会有残差结构，把embdding和输出相加，而且会进行层归一化LayerNorm，可以防止过拟合

18. Transformer 的两个mask机制是什么？
a. 其实transformer有三个mask，一个是在encoder的自注意力层，可以让模型把pad的值给mask掉，处理变长序列；其余两个是在decoder的两个自注意力层，第一个自注意力层的mask是为了让mask掉未来的词，让模型只能看到过去和当前的词，保证训练，第二个自注意力层，也是为了处理变长序列，mask掉pad值

**19**. Transformer 为什么要用LayerNorm，而不是BatchNorm，作用是什么？
a. 因为Transformer模型是处理序列数据，所以需要用LayerNorm，而不是BatchNorm，作用是防止模型过拟合

20. Encoder和Decoder是如何进行交互的？
a. encoder的输出会作为decoder第二个自注意力层的输入，并与wk和wv相乘，得到k，v



进阶：
21. 如何提高Transformer模型中自注意力机制的计算效率？
？？
a. 通过使用稀疏注意力机制，只计算部分注意力分数，而不是所有可能的注意力分数，可以提高计算效率
b. 通过使用量化技术，将注意力分数的精度从32位降低到8位或16位，可以减少计算量
c. 通过使用混合精度训练，在训练过程中使用16位浮点数进行前向传播和反向传播，而在推理过程中使用32位浮点数进行推理，可以减少计算量
d. 通过使用注意力蒸馏技术，将注意力分数的计算过程分解为多个步骤，可以减少计算量
e. 通过使用注意力机制的优化技术，如局部注意力机制、稀疏注意力机制等，可以减少计算量

22. 为什么自注意力层要除以根号d_k？有方法可以不除吗？
a. 因为QK^T的值会很大，所以需要除以根号d_k，来防止梯度消失
b. ？

23. Transformer 模型中注意力权重如何解释模型的决策？
a. ？？注意力权重可以解释模型

24. 如何在自注意力机制中平衡局部信息和全局信息的捕获？
a. ？？

25. 基于attention 有哪些代表性的改进方法？分别针对的是什么问题？
a. kvcache，解决计算重复，提高计算效率，但是相应的需要缓存encoder的输出，会占更大显存
b.

26. 如何设计更有效的注意力机制来处理层次化或结构化数据？
a. ???

27. 如何量化并优化注意力机制的计算成本与性能之间的权衡？
a. ???

28. 注意力机制如何适应不同任务的动态性和多样性？
？？

29. 自注意力机制能否帮助模型理解更复杂的语言现象，比如讽刺、隐喻、双关？
？？

30. 如何结合注意力机制与其他类型的神经网络模块，比如cnn或rnn？
？？



. 在Transformer模型中，残差连接(residual connections)和层归一化(layer normalization)的作用是什么？
a. 残差连接的作用是防止模型过拟合，层归一化是防止模型过拟合

. 在Transformer模型中，feed-forward网络的作用是什么？
a. feed-forward网络的作用是给每个词一个位置信息，因为Transformer模型没有RNN，所以需要位置编码来告诉模型每个词的位置
